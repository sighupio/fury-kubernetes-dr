# Copyright (c) 2020 SIGHUP s.r.l All rights reserved.
# Use of this source code is governed by a BSD-style
# license that can be found in the LICENSE file.

name: license
kind: pipeline
type: docker

steps:
  - name: check
    image: docker.io/library/golang:1.21
    pull: always
    commands:
      - go install github.com/google/addlicense@v1.1.1
      - addlicense -c "SIGHUP s.r.l" -v -l bsd --check .
---
name: policeman
kind: pipeline
type: docker

depends_on:
  - license

platform:
  os: linux
  arch: amd64

steps:
  - name: lint
    image: quay.io/sighup/policeman
    pull: always
    environment:
      VALIDATE_TERRAFORM_TERRASCAN: "false"
      # Identifies false positives like missing 'selector'.
      # Doing this is valid for Kustomize patches
      VALIDATE_KUBERNETES_KUBEVAL: "false"
      # Some duplicated code is intended.
      VALIDATE_JSCPD: "false"
      VALIDATE_DOCKERFILE: "false"
      # Disable natural language checks
      VALIDATE_NATURAL_LANGUAGE: "false"
    depends_on:
      - clone

  - name: render
    image: quay.io/sighup/e2e-testing:1.1.0_0.2.2_2.16.1_1.9.4_1.20.7_3.8.7_2.4.1
    pull: always
    depends_on:
      - clone
    commands:
      - kustomize build katalog/velero/velero-on-prem > velero.yml

  - name: check-deprecated-apis
    image: us-docker.pkg.dev/fairwinds-ops/oss/pluto:v5
    pull: always
    depends_on:
      - render
    commands:
      # we use --ignore-deprecations because we don't want the CI to fail when the API has not been removed yet.
      - /pluto detect velero.yml --target-versions=k8s=v1.31.0 --ignore-deprecations

---
name: e2e-kubernetes-1.28
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    depends_on: [clone]
    environment:
      CLUSTER_VERSION: v1.28.0
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      # /drone/src is the default workdir for the pipeline
      # using this folder we don't need to mount another
      # shared volume between the steps
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      # NOTE: kind's `--wait` flag that waits for the control-plane ot be ready
      # does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # save the kubeconfig so we can use it from other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    # KUBECTL 1.29.1 - KUSTOMIZE 3.5.3 - HELM 3.1.1 - YQ 4.21.1 - ISTIOCTL 1.9.4 - FURYCTL 0.9.0 - BATS 1.1.0
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ create-kind-cluster ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ test-install ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh
      - bats -t katalog/tests/velero/velero-backup-with-restic-test.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      CI_PIPELINE_NUMBER: dr-128-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_128.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_128.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_128.yaml

  - name: apply-aws-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_128.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_128.yaml -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-aws-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      CI_PIPELINE_NUMBER: dr-128-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
      - success
      - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-128.json
      CI_PIPELINE_NUMBER: dr-128-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_128.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_128.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_128.yaml

  - name: apply-gcp-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_128.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_128.yaml -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-gcp-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-128.json
      CI_PIPELINE_NUMBER: dr-128-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
      - success
      - failure

  - name: init-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      CI_PIPELINE_NUMBER: k128a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_128.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_128.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_128.yaml

  - name: apply-azure-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_128.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_128.yaml -n kube-system
      - kustomize build katalog/velero/velero-azure | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-azure
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-azure-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
      KUBECONFIG: /drone/src/kubeconfig-128
      CI_PIPELINE_NUMBER: k128a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
    when:
      status:
      - success
      - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    pull: always
    depends_on: [destroy-azure]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-128
    commands:
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock

---
name: e2e-kubernetes-1.29
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    depends_on: [clone]
    environment:
      CLUSTER_VERSION: v1.29.0
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      # /drone/src is the default workdir for the pipeline
      # using this folder we don't need to mount another
      # shared volume between the steps
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      # NOTE: kind's `--wait` flag that waits for the control-plane ot be ready
      # does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # save the kubeconfig so we can use it from other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    # KUBECTL 1.29.1 - KUSTOMIZE 3.5.3 - HELM 3.1.1 - YQ 4.21.1 - ISTIOCTL 1.9.4 - FURYCTL 0.9.0 - BATS 1.1.0
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ create-kind-cluster ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ test-install ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh
      - bats -t katalog/tests/velero/velero-backup-with-restic-test.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-aws-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-aws-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
      - success
      - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-129-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-gcp-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-gcp-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-129-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
      - success
      - failure

  - name: init-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      CI_PIPELINE_NUMBER: k129a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-azure-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-azure | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-azure
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-azure-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
      KUBECONFIG: /drone/src/kubeconfig-129
      CI_PIPELINE_NUMBER: k129a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
    when:
      status:
      - success
      - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    pull: always
    depends_on: [destroy-azure]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
    commands:
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock

---
name: e2e-kubernetes-1.30
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    depends_on: [clone]
    environment:
      CLUSTER_VERSION: v1.30.4
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-130
      # /drone/src is the default workdir for the pipeline
      # using this folder we don't need to mount another
      # shared volume between the steps
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      # NOTE: kind's `--wait` flag that waits for the control-plane ot be ready
      # does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # save the kubeconfig so we can use it from other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    # KUBECTL 1.29.1 - KUSTOMIZE 3.5.3 - HELM 3.1.1 - YQ 4.21.1 - ISTIOCTL 1.9.4 - FURYCTL 0.9.0 - BATS 1.1.0
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ create-kind-cluster ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ test-install ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh
      - bats -t katalog/tests/velero/velero-backup-with-restic-test.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-aws-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-aws-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
      - success
      - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-130-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-gcp-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-gcp-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-129-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
      - success
      - failure

  - name: init-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      CI_PIPELINE_NUMBER: k130a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-azure-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-azure | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-azure
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-azure-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.30.4
      KUBECONFIG: /drone/src/kubeconfig-130
      CI_PIPELINE_NUMBER: k130a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
    when:
      status:
      - success
      - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    pull: always
    depends_on: [destroy-azure]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-129
    commands:
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock

---
name: e2e-kubernetes-1.31
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.29.1_3.10.0
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    depends_on: [clone]
    environment:
      CLUSTER_VERSION: v1.31.1
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-131
      # /drone/src is the default workdir for the pipeline
      # using this folder we don't need to mount another
      # shared volume between the steps
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      # NOTE: kind's `--wait` flag that waits for the control-plane ot be ready
      # does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # save the kubeconfig so we can use it from other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    # KUBECTL 1.29.1 - KUSTOMIZE 3.5.3 - HELM 3.1.1 - YQ 4.21.1 - ISTIOCTL 1.9.4 - FURYCTL 0.9.0 - BATS 1.1.0
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ create-kind-cluster ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-131
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ test-install ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh
      - bats -t katalog/tests/velero/velero-backup-with-restic-test.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-aws-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-aws-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      CI_PIPELINE_NUMBER: dr-129-aws
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
      - success
      - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-aws ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-129-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-gcp-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-gcp-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      GCP_CREDENTIALS_PATH: /drone/src/terraform-credentials-129.json
      CI_PIPELINE_NUMBER: dr-129-gcp
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GCP_CREDENTIALS_PATH}
      - export GOOGLE_APPLICATION_CREDENTIALS=$${GCP_CREDENTIALS_PATH}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}-${CI_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
      - success
      - failure

  - name: init-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-gcp ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      CI_PIPELINE_NUMBER: k131a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
      - terraform output -raw cloud_credentials > /drone/src/cloud_credentials_129.yaml
      - terraform output -raw volume_snapshot_location > /drone/src/volume_snapshot_location_129.yaml
      - terraform output -raw backup_storage_location > /drone/src/backup_storage_location_129.yaml

  - name: apply-azure-configuration
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ init-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - kubectl apply -f /drone/src/cloud_credentials_129.yaml -n kube-system
      - kubectl apply -f /drone/src/backup_storage_location_129.yaml -n kube-system
      - kustomize build katalog/velero/velero-azure | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - sleep 30 # wait for velero to restart
      - kubectl get pods -n kube-system

  - name: test-backup-restore-azure
    image: quay.io/sighup/e2e-testing:1.1.0_0.11.0_3.12.0_1.9.4_1.29.1_3.5.3_4.33.3
    pull: always
    network_mode: host
    depends_on: [ apply-azure-configuration ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
    commands:
      - curl -Ls -o velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
      - tar -zxf velero.tar.gz
      - mv velero*/velero /usr/local/bin/velero
      - bats -t katalog/tests/velero/velero-backup.sh

  - name: destroy-azure
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [ test-backup-restore-azure ]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
      KUBECONFIG: /drone/src/kubeconfig-131
      CI_PIPELINE_NUMBER: k131a
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: arm_terraform_tf_states_bucket_name
      STORAGE_ACCOUNT_NAME:
        from_secret: storage_account_name
      RESOURCE_GROUP_NAME:
        from_secret: resource_group_name
      ARM_CLIENT_ID:
        from_secret: arm_client_id
      ARM_CLIENT_SECRET:
        from_secret: arm_client_secret
      ARM_SUBSCRIPTION_ID:
        from_secret: arm_subscription_id
      ARM_TENANT_ID:
        from_secret: arm_tenant_id
    commands:
      - cd examples/azure-example
      - terraform init
        --backend=true
        --backend-config="storage_account_name=$${STORAGE_ACCOUNT_NAME}"
        --backend-config="resource_group_name=$${RESOURCE_GROUP_NAME}"
        --backend-config="container_name=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${CI_REPO}/${DRONE_BRANCH}/${CI_BUILD_NUMBER}/$${CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CI_PIPELINE_NUMBER}${CI_BUILD_NUMBER}"
    when:
      status:
      - success
      - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/dind-kind-kubectl-kustomize:0.20.0_1.31.1_3.10.0
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    pull: always
    depends_on: [destroy-azure]
    environment:
      CLUSTER_NAME: ${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.1
    commands:
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock

---
name: release
kind: pipeline
type: docker

depends_on:
  - e2e-kubernetes-1.30
  - e2e-kubernetes-1.29
  - e2e-kubernetes-1.30
  - e2e-kubernetes-1.31

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

steps:
  - name: prepare-tar-gz
    image: alpine:latest
    pull: always
    depends_on: [ clone ]
    commands:
      - tar -zcvf fury-kubernetes-dr-${DRONE_TAG}.tar.gz katalog/ LICENSE README.md
    when:
      ref:
        include:
          - refs/tags/**

  - name: prepare-release-notes
    image: quay.io/sighup/fury-release-notes-plugin:3.7_2.8.4
    pull: always
    depends_on: [ clone ]
    settings:
      release_notes_file_path: release-notes.md
    when:
      ref:
        include:
          - refs/tags/**

  - name: publish-prerelease
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-tar-gz
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: overwrite
      files:
        - fury-kubernetes-dr-${DRONE_TAG}.tar.gz
      prerelease: true
      overwrite: true
      title: "Preview ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        include:
          - refs/tags/v**-rc**

  - name: publish-stable
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-tar-gz
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: overwrite
      files:
        - fury-kubernetes-dr-${DRONE_TAG}.tar.gz
      prerelease: false
      overwrite: true
      title: "Release ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        exclude:
          - refs/tags/v**-rc**
        include:
          - refs/tags/v**
